{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tzJwGiCmbSm"
      },
      "source": [
        "Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkSKRksRa5Z4",
        "outputId": "abd48ec2-f496-4c12-bfd5-ab9e71561d69"
      },
      "source": [
        "!git clone https://github.com/misbah4064/facial_expressions.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'facial_expressions'...\n",
            "remote: Enumerating objects: 14243, done.\u001b[K\n",
            "remote: Total 14243 (delta 0), reused 0 (delta 0), pack-reused 14243\u001b[K\n",
            "Receiving objects: 100% (14243/14243), 240.06 MiB | 33.55 MiB/s, done.\n",
            "Resolving deltas: 100% (232/232), done.\n",
            "Checking out files: 100% (14004/14004), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUngMM9tZfVl",
        "outputId": "22b80288-b2e0-4263-8069-09f713cb6347"
      },
      "source": [
        "%cd facial_expressions/\n",
        "%mkdir -p data_set/{anger,happy,neutral,sad,surprise}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/facial_expressions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reSb3qDnm4nW"
      },
      "source": [
        "Extracting Images with expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je-nge71Z2oG",
        "outputId": "820b890c-2c4e-4236-9ff1-1c82be055237"
      },
      "source": [
        "import cv2\n",
        "with open('anger.txt','r') as f:\n",
        "    img = [line.strip() for line in f]\n",
        "for image in img:\n",
        "    loadedImage = cv2.imread(\"images/\"+image)\n",
        "    cv2.imwrite(\"data_set/anger/\"+image,loadedImage)\n",
        "print(\"done writing\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done writing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoAXNkRekwFo"
      },
      "source": [
        "%mkdir dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPr0qNb-meCV"
      },
      "source": [
        " Creating Data Set of Faces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI0V9JTIix-Q",
        "outputId": "39f92741-3332-4067-edce-d9ed58180051"
      },
      "source": [
        "import cv2\n",
        "\n",
        "with open('anger.txt','r') as f:\n",
        "    images = [line.strip() for line in f]\n",
        "\n",
        "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "# For each Emotion, enter one numeric face id\n",
        "face_id = input('\\n Enter Emotion id end press <return> ==>  ')\n",
        "\n",
        "count = 0\n",
        "\n",
        "for image in images:\n",
        "    img = cv2.imread(\"data_set/anger/\"+image)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "\n",
        "        cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)     \n",
        "        count += 1\n",
        "\n",
        "        # Save the captured image into the datasets folder\n",
        "        cv2.imwrite(\"dataset/User.\" + str(face_id) + '.' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n",
        "\n",
        "print(\"\\n Done creating face data\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Enter Emotion id end press <return> ==>  0\n",
            "\n",
            " Done creating face data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKBWdnx_koxg"
      },
      "source": [
        "%mkdir trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7k2Yg1jmqT2"
      },
      "source": [
        "Training Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODFPp__cjAXl",
        "outputId": "2e58d060-732f-45b3-fd39-7540b351340b"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Path for face image database\n",
        "path = 'dataset'\n",
        "\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "detector = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\");\n",
        "\n",
        "# function to get the images and label data\n",
        "def getImagesAndLabels(path):\n",
        "\n",
        "    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]    \n",
        "    faceSamples=[]\n",
        "    ids = []\n",
        "\n",
        "    for imagePath in imagePaths:\n",
        "\n",
        "        PIL_img = Image.open(imagePath).convert('L') # convert it to grayscale\n",
        "        img_numpy = np.array(PIL_img,'uint8')\n",
        "\n",
        "        id = int(os.path.split(imagePath)[-1].split(\".\")[1])\n",
        "        faces = detector.detectMultiScale(img_numpy)\n",
        "\n",
        "        for (x,y,w,h) in faces:\n",
        "            faceSamples.append(img_numpy[y:y+h,x:x+w])\n",
        "            ids.append(id)\n",
        "\n",
        "    return faceSamples,ids\n",
        "\n",
        "print (\"\\n [INFO] Training faces....\")\n",
        "faces,ids = getImagesAndLabels(path)\n",
        "recognizer.train(faces, np.array(ids))\n",
        "\n",
        "# Save the model into trainer/trainer.yml\n",
        "recognizer.write('trainer/trainer.yml') \n",
        "\n",
        "# Print the numer of Emotions trained and end program\n",
        "print(\"\\n [INFO] {0} Emotions trained. Exiting Program\".format(len(np.unique(ids))))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " [INFO] Training faces....\n",
            "\n",
            " [INFO] 1 Emotions trained. Exiting Program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_XDgmYGnFse"
      },
      "source": [
        "Recognition (Testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqX616mtk2vN",
        "outputId": "228977b3-d050-4899-a956-62b3ed073df4"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os \n",
        "\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
        "recognizer.read('trainer/trainer.yml')\n",
        "cascadePath = \"haarcascade_frontalface_default.xml\"\n",
        "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "#iniciate id counter\n",
        "id = 0\n",
        "\n",
        "# Emotions related to ids: example ==> Anger: id=0,  etc\n",
        "names = ['Anger', 'Happy', 'None', 'None', 'None', 'None'] \n",
        "\n",
        "# Initialize and start realtime video capture\n",
        "cam = cv2.VideoCapture(0)\n",
        "cam.set(3, 640) # set video widht\n",
        "cam.set(4, 480) # set video height\n",
        "\n",
        "# Define min window size to be recognized as a face\n",
        "minW = 0.1*cam.get(3)\n",
        "minH = 0.1*cam.get(4)\n",
        "\n",
        "# ret, img =cam.read()\n",
        "img = cv2.imread(\"dwayne.jpg\")\n",
        "# img = cv2.flip(img, -1) # Flip vertically\n",
        "\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "faces = faceCascade.detectMultiScale( \n",
        "    gray,\n",
        "    scaleFactor = 1.2,\n",
        "    minNeighbors = 5,\n",
        "    minSize = (int(minW), int(minH)),\n",
        "    )\n",
        "\n",
        "for(x,y,w,h) in faces:\n",
        "\n",
        "    cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
        "\n",
        "    id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
        "\n",
        "    # Check if confidence is less them 100 ==> \"0\" is perfect match \n",
        "    if (confidence < 100):\n",
        "        id = names[id]\n",
        "        confidence = \"  {0}%\".format(round(100 - confidence))\n",
        "    else:\n",
        "        id = \"unknown\"\n",
        "        confidence = \"  {0}%\".format(round(100 - confidence))\n",
        "    \n",
        "    cv2.putText(img, str(id), (x+5,y-5), font, 1, (255,255,255), 2)\n",
        "    cv2.putText(img, str(confidence), (x+5,y+h-5), font, 1, (255,255,0), 1)  \n",
        "\n",
        "cv2.imwrite(\"dwayne_jon.jpg\",img) \n",
        "\n",
        "print(\"\\n [INFO] Done detecting and Image is saved\")\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " [INFO] Done detecting and Image is saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJB_cosqnH2t"
      },
      "source": [
        "Display Detected Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeFb_TgImUoW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "EsLpgpgbk_Q_",
        "outputId": "f0323b4b-5aa4-4915-9fd7-12ed6b7783f9"
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "image = cv2.imread(\"elon_musk.jpg\")\n",
        "height, width = image.shape[:2]\n",
        "resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(18, 10)\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8173f5d43c76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"elon_musk.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ]
}